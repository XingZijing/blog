---
title: ISMIR2020
author: Pablo Alonso
date: 2020-10-23 20:00:00 +0800
categories: [Conferences]
tags: [MIR]
---


ISMIR 2020 has been a great conference for me. As a second-time attendee, it was very close to last year's experience in terms of the number of interesting projects and ideas I've learned of.
Although, of course, I missed the social aspect very much, and the opportunity to visit Montreal!

One of my highlights was the [keynote](https://www.youtube.com/watch?v=a3LutlLru5k) by Safiya Noble about Big Tech brilliantly framed historically in the context of Big Cotton or Big Tobacco.
Another interesting event was the Validity in MIR meet-up, with a nice discussion about experimental design and statistical approaches to validate the models we develop in the lab before they are freed into the wild. Some of these ideas are going to be very relevant for my first steps in the PhD.
<!-- It's great -->

After a couple of days processing all the information, I have come up with a selection of the papers that I enjoyed the most, just in the order they appeared in my notes:

- **Semi-supervised Learning Using Teacher-student Models for Vocal Melody Extraction**  
*Sangeun Kum, Jing-Hua Lin, Li Su and Juhan Nam*  
An application of the teacher-student paradigm for melody extraction.
Training in predictions generated by a supervised teacher network helps a (noisy) student to improve performance on unseen datasets.

- **The Freesound Loop Dataset and Annotation Tool**  
*Antonio Ramires, Frederic Font, Dmitry Bogdanov, Jordan B. L. Smith, Yi-Hsuan Yang, Joann Ching, Bo-Yu Chen, Yueh-Kao Wu, Hsu Wei-Han and Xavier Serra*  
A multi-class dataset for audio loops with an online annotation tool.
In our late-breaking demo [1], we used a single-label subset of it to train a loop musical role classifier.
If you like this work, you can help with the [ongoing annotation process](http://mtg.upf.edu/fslannotator/)!

- **Can't Trust the Feeling? How Open Data Reveals Unexpected Behavior of High-level Music Descriptors**  
*Cynthia C. S. Liem and Chris Mostert*  
Interesting work assessing the [AcousticBrainz ](https://acousticbrainz.org/datasets/accuracy) SVM models.
The conclusion is pretty aligned with previous work evidencing the lack of generalization of those models [2].
This work goes further and points to certain aspects that correlate with the predictions such as the Essentia version used to extract the features, the audio codec, or the bit depth.

- **Metric Learning vs Classification for Disentangled Music Representation Learning**  
*Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin and Juhan Nam*  
This work compares metric learning, classification, and proxy-based models with and without an embedding disentanglement mechanism in various downstream tasks.
The models based on classification tasks are superior and the disentanglement helps to achieve a new state of the art for auto-tagging.

- **Drumgan: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks**  
*Javier Nistal, S. Lattner and G. Richard*  
Really nice sounding drum hits generated with GANs and conditioned on [AudioCommons timbral attributes](https://www.audiocommons.org/2018/09/05/timbre-sound.html).
The sound quality doesn't degrade even under extreme conditioning parameters.
According to the author, this may be due to noisy input which prevents the model to overreact to unseen conditioning values.

- **Ultra-light Deep MIR by Trimming Lottery Tickets**  
*Philippe Esling, Theis Bazin, Adrien Bitton, Tristan Carsault and Ninon Devis*  
A simple technique that allows a great model compression in terms of compute.
Very relevant for the deployment of real-time audio/music models.

- **Essentia.js: a JavaScript Library for Music and Audio Analysis on the Web**  
*Albin Correya, Dmitry Bogdanov, Luis Joglar-Ongay and Xavier Serra*  
Work produced by my colleges which obtained the **Best Reproducibility** award!
It presents the integration of Essentia into JavaScript which opens the door to scalable, serverless MIR in the browser.

- **Deconstruct, Analyse, Reconstruct: How to Improve Tempo, Beat, and Downbeat Estimation**  
*Sebastian Böck and Matthew Davies*  
This work revisits a previous beat-tracking model enhancing the CNN architecture and applying data augmentation to achieve a new state of the art in the task.

- **Multilingual Music Genre Embeddings for Effective Cross-lingual Music Item Annotation**  
*Elena V. Epure, Guillaume Salha and Romain Hennequin*  
They learn multilingual genre embeddings without a parallel corpus. The method proves to be useful for translating the genres between different languages and taxonomies. I believe that this technique can be very useful for leveraging genre datasets with slightly divergent taxonomies.

- **Joyful for You and Tender for Us: the Influence of Individual Characteristics and Language on Emotion Labeling and Classification**  
*Juan Sebastián Gómez-Cañón, Estefanía Cano, Perfecto Herrera and Emilia Gómez*  
In MIR, emotion-related classifiers are typically trained assuming that a track labeled as `sad` is as objective as a picture labeled as `apple` in CV tasks. This work finds that your familiarity with the track, your ability to understand lyrics, your musical knowledge, or your mother tongue have an impact on how you perceive emotion labels, especially for the most complex ones.
<!-- I find this work very relevant in the context of our mood classifiers [1,3]. -->

- **"Butter Lyrics Over Hominy Grit": Comparing Audio and Psychology-based Text Features in MIR Tasks**  
*Jaehun Kim, Andrew M. Demetriou, Sandy Manolios, M. Stella Tavella and Cynthia C. S. Liem*  
Interesting work evaluating psychology inspired features extracted from the lyrics on common MIR tasks showing promising results.
I believe that the message on the lyrics (or even its aesthetic codes) are a powerful (yet cheap) source of information to explain many of the human ways of understanding music. Nice to see work in that direction.



## References
[1] Alonso-Jiménez, P., Bogdanov, D., & Serra, X. (2020). **Deep embeddings with Essentia models**. In *21th Conference of the International Society for Music Information Retrieval* (ISMIR2020). – **Late-breaking demo**

[2] Bogdanov, D., Porter, A., Boyer, H., & Serra, X. (2016). **Cross-collection evaluation for music classification tasks.** In *Proceedings of the 17th International Society for Music Information Retrieval Conference* (ISMIR2016).


[3] - Alonso-Jiménez, P., Bogdanov, D., Pons, J., & Serra, X. (2020). **TensorFlow Audio Models in Essentia**. In *45th IEEE International Conference on Acoustics, Speech and Signal Processing* (ICASSP2020).
